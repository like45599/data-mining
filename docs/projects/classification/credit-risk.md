# ä¿¡ç”¨é£é™©è¯„ä¼°

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">ğŸ“š</span>é¡¹ç›®æ¦‚è¿°
  </div>
  <div class="knowledge-card__content">
    <ul>
      <li><strong>éš¾åº¦</strong>ï¼šé«˜çº§</li>
      <li><strong>ç±»å‹</strong>ï¼šåˆ†ç±»</li>
      <!-- <li><strong>é¢„è®¡æ—¶é—´</strong>ï¼š6-8å°æ—¶</li> -->
      <li><strong>æŠ€èƒ½ç‚¹</strong>ï¼šä¸å¹³è¡¡æ•°æ®å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è§£é‡Šã€è¯„ä¼°æŒ‡æ ‡é€‰æ‹©</li>
      <li><strong>å¯¹åº”çŸ¥è¯†æ¨¡å—</strong>ï¼š<a href="/core/classification/svm.html">åˆ†ç±»ç®—æ³•</a></li>
    </ul>
  </div>
</div>

## é¡¹ç›®èƒŒæ™¯

ä¿¡ç”¨é£é™©è¯„ä¼°æ˜¯é‡‘èæœºæ„çš„æ ¸å¿ƒä¸šåŠ¡ä¹‹ä¸€ï¼Œç”¨äºåˆ¤æ–­å€Ÿæ¬¾äººæ˜¯å¦æœ‰èƒ½åŠ›æŒ‰æ—¶å¿è¿˜è´·æ¬¾ã€‚å‡†ç¡®çš„ä¿¡ç”¨é£é™©è¯„ä¼°å¯ä»¥å¸®åŠ©é‡‘èæœºæ„é™ä½åè´¦ç‡ï¼ŒåŒæ—¶ä¸ºæ›´å¤šåˆæ ¼çš„å€Ÿæ¬¾äººæä¾›é‡‘èæœåŠ¡ã€‚

ä¼ ç»Ÿçš„ä¿¡ç”¨è¯„åˆ†æ¨¡å‹ä¸»è¦ä¾èµ–äºå€Ÿæ¬¾äººçš„ä¿¡ç”¨å†å²ã€æ”¶å…¥æ°´å¹³å’Œå€ºåŠ¡æ¯”ç‡ç­‰å› ç´ ã€‚éšç€å¤§æ•°æ®å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„å‘å±•ï¼Œç°ä»£ä¿¡ç”¨é£é™©è¯„ä¼°å¯ä»¥æ•´åˆæ›´å¤šç»´åº¦çš„æ•°æ®ï¼ŒåŒ…æ‹¬äº¤æ˜“è¡Œä¸ºã€ç¤¾äº¤ç½‘ç»œå’Œå¿ƒç†ç‰¹å¾ç­‰ï¼Œæ„å»ºæ›´å…¨é¢å’Œå‡†ç¡®çš„é£é™©é¢„æµ‹æ¨¡å‹ã€‚

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¾·å›½ä¿¡ç”¨æ•°æ®é›†ï¼Œæ„å»ºä¸€ä¸ªä¿¡ç”¨é£é™©è¯„ä¼°æ¨¡å‹ï¼Œé¢„æµ‹å€Ÿæ¬¾äººæ˜¯å¦ä¼šè¿çº¦ã€‚

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">ğŸ’¡</span>ä½ çŸ¥é“å—ï¼Ÿ
  </div>
  <div class="knowledge-card__content">
    <p>ä¿¡ç”¨è¯„åˆ†çš„æ¦‚å¿µæœ€æ—©ç”±ç¾å›½å…¬å¹³è‰¾è¨å…‹å…¬å¸(Fair Isaac Corporation)åœ¨1950å¹´ä»£æå‡ºï¼Œå‘å±•æˆä¸ºä»Šå¤©å¹¿æ³›ä½¿ç”¨çš„FICOè¯„åˆ†ç³»ç»Ÿã€‚ç°ä»£ä¿¡ç”¨è¯„åˆ†ç³»ç»Ÿé€šå¸¸å°†å€Ÿæ¬¾äººçš„ä¿¡ç”¨çŠ¶å†µé‡åŒ–ä¸º300-850ä¹‹é—´çš„åˆ†æ•°ï¼Œåˆ†æ•°è¶Šé«˜è¡¨ç¤ºä¿¡ç”¨é£é™©è¶Šä½ã€‚</p>
  </div>
</div>

## æ•°æ®é›†ä»‹ç»

å¾·å›½ä¿¡ç”¨æ•°æ®é›†åŒ…å«1000åå€Ÿæ¬¾äººçš„ä¿¡æ¯ï¼Œæ¯ä¸ªå€Ÿæ¬¾äººæœ‰20ä¸ªç‰¹å¾ï¼ŒåŒ…æ‹¬ï¼š

- ä¸ªäººä¿¡æ¯ï¼šå¹´é¾„ã€æ€§åˆ«ã€å©šå§»çŠ¶å†µç­‰
- è´¢åŠ¡çŠ¶å†µï¼šæ”¶å…¥ã€å‚¨è“„ã€ç°æœ‰ä¿¡è´·æ•°é‡ç­‰
- å°±ä¸šä¿¡æ¯ï¼šå°±ä¸šç±»å‹ã€å°±ä¸šå¹´é™ç­‰
- ä½æˆ¿ä¿¡æ¯ï¼šä½æˆ¿ç±»å‹ã€å±…ä½æ—¶é—´ç­‰
- ä¿¡ç”¨å†å²ï¼šè¿‡å¾€ä¿¡ç”¨è®°å½•ã€è´·æ¬¾ç›®çš„ç­‰

ç›®æ ‡å˜é‡æ˜¯äºŒå…ƒåˆ†ç±»ï¼šå¥½å®¢æˆ·(0)æˆ–åå®¢æˆ·(1)ï¼Œå…¶ä¸­åå®¢æˆ·è¡¨ç¤ºå­˜åœ¨è¿çº¦é£é™©ã€‚

æ•°æ®é›†çš„ä¸€ä¸ªé‡è¦ç‰¹ç‚¹æ˜¯ç±»åˆ«ä¸å¹³è¡¡ï¼Œå¥½å®¢æˆ·å 70%ï¼Œåå®¢æˆ·å 30%ã€‚è¿™ç§ä¸å¹³è¡¡åæ˜ äº†ç°å®ä¸–ç•Œä¸­çš„ä¿¡ç”¨é£é™©åˆ†å¸ƒï¼Œä½†ä¹Ÿç»™æ¨¡å‹è®­ç»ƒå¸¦æ¥äº†æŒ‘æˆ˜ã€‚

## é¡¹ç›®ç›®æ ‡

1. æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹å€Ÿæ¬¾äººä¿¡ç”¨é£é™©çš„åˆ†ç±»æ¨¡å‹
2. å¤„ç†æ•°æ®é›†ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
3. è¯†åˆ«å½±å“ä¿¡ç”¨é£é™©çš„å…³é”®å› ç´ 
4. è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¯„ä¼°æŒ‡æ ‡ä¸‹çš„è¡¨ç°
5. æä¾›æ¨¡å‹è§£é‡Šå’Œä¸šåŠ¡å»ºè®®

## å®æ–½æ­¥éª¤

### 1. æ•°æ®æ¢ç´¢ä¸é¢„å¤„ç†

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦äº†è§£æ•°æ®çš„åŸºæœ¬ç‰¹å¾å’Œè´¨é‡ï¼š

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# åŠ è½½æ•°æ®
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data"
column_names = ['status', 'duration', 'credit_history', 'purpose', 'amount', 
                'savings', 'employment_duration', 'installment_rate', 'personal_status_sex', 
                'other_debtors', 'present_residence', 'property', 'age', 'other_installment_plans', 
                'housing', 'existing_credits', 'job', 'num_dependents', 'telephone', 'foreign_worker', 'target']
df = pd.read_csv(url, sep=' ', header=None, names=column_names)

# å°†ç›®æ ‡å˜é‡è½¬æ¢ä¸º0å’Œ1ï¼ˆåŸå§‹æ•°æ®ä¸­æ˜¯1å’Œ2ï¼‰
df['target'] = df['target'].map({1: 0, 2: 1})

# æŸ¥çœ‹æ•°æ®åŸºæœ¬ä¿¡æ¯
print(df.info())
print(df.describe())

# æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
print(df['target'].value_counts(normalize=True))

# å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
plt.figure(figsize=(8, 6))
sns.countplot(x='target', data=df)
plt.title('ä¿¡ç”¨é£é™©åˆ†å¸ƒ')
plt.xlabel('é£é™©ç±»åˆ« (0=å¥½å®¢æˆ·, 1=åå®¢æˆ·)')
plt.ylabel('æ•°é‡')
plt.show()

# æ¢ç´¢æ•°å€¼ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„å…³ç³»
numerical_features = ['duration', 'amount', 'age', 'existing_credits']
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

for i, feature in enumerate(numerical_features):
    sns.boxplot(x='target', y=feature, data=df, ax=axes[i])
    axes[i].set_title(f'{feature} vs ä¿¡ç”¨é£é™©')
    axes[i].set_xlabel('é£é™©ç±»åˆ« (0=å¥½å®¢æˆ·, 1=åå®¢æˆ·)')

plt.tight_layout()
plt.show()

# æ¢ç´¢ç±»åˆ«ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„å…³ç³»
categorical_features = ['credit_history', 'purpose', 'personal_status_sex', 'savings']
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

for i, feature in enumerate(categorical_features):
    cross_tab = pd.crosstab(df[feature], df['target'], normalize='index')
    cross_tab.plot(kind='bar', stacked=True, ax=axes[i])
    axes[i].set_title(f'{feature} vs ä¿¡ç”¨é£é™©')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('æ¯”ä¾‹')
    axes[i].legend(['å¥½å®¢æˆ·', 'åå®¢æˆ·'])

plt.tight_layout()
plt.show()
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†æ•°æ®ä¸­çš„ç±»åˆ«ç‰¹å¾å’Œæ•°å€¼ç‰¹å¾ï¼š

```python
# è¯†åˆ«æ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾
numerical_features = ['duration', 'amount', 'installment_rate', 'present_residence', 
                      'age', 'existing_credits', 'num_dependents']
categorical_features = ['status', 'credit_history', 'purpose', 'savings', 
                        'employment_duration', 'personal_status_sex', 'other_debtors', 
                        'property', 'other_installment_plans', 'housing', 'job', 
                        'telephone', 'foreign_worker']

# åˆ†å‰²æ•°æ®
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# åˆ›å»ºé¢„å¤„ç†ç®¡é“
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ])

# åº”ç”¨é¢„å¤„ç†
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# æ£€æŸ¥å¤„ç†åçš„ç‰¹å¾ç»´åº¦
print(f"å¤„ç†åçš„è®­ç»ƒé›†å½¢çŠ¶: {X_train_processed.shape}")
```

### 2. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡

ä¿¡ç”¨é£é™©æ•°æ®é€šå¸¸å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šç§æ–¹æ³•å¤„ç†ï¼š

```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
from collections import Counter

# æŸ¥çœ‹åŸå§‹ç±»åˆ«åˆ†å¸ƒ
print(f"åŸå§‹è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: {Counter(y_train)}")

# ä½¿ç”¨SMOTEè¿‡é‡‡æ ·
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)
print(f"SMOTEåè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: {Counter(y_train_smote)}")

# ä½¿ç”¨éšæœºæ¬ é‡‡æ ·
rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus = rus.fit_resample(X_train_processed, y_train)
print(f"æ¬ é‡‡æ ·åè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: {Counter(y_train_rus)}")

# ä½¿ç”¨ç»„åˆé‡‡æ ·ï¼ˆSMOTE + æ¬ é‡‡æ ·ï¼‰
over = SMOTE(sampling_strategy=0.5, random_state=42)  # å°†å°‘æ•°ç±»è¿‡é‡‡æ ·åˆ°å¤šæ•°ç±»çš„50%
under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)  # å°†å¤šæ•°ç±»æ¬ é‡‡æ ·åˆ°å°‘æ•°ç±»çš„80%
combined_sampling = ImbPipeline(steps=[('over', over), ('under', under)])
X_train_combined, y_train_combined = combined_sampling.fit_resample(X_train_processed, y_train)
print(f"ç»„åˆé‡‡æ ·åè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: {Counter(y_train_combined)}")
```

### 3. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°

æˆ‘ä»¬å°†å°è¯•å¤šç§åˆ†ç±»ç®—æ³•ï¼Œå¹¶ä½¿ç”¨é€‚åˆä¸å¹³è¡¡æ•°æ®çš„è¯„ä¼°æŒ‡æ ‡ï¼š

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve
from sklearn.model_selection import cross_val_score

# å®šä¹‰è¯„ä¼°å‡½æ•°
def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print(f"\n{model_name} è¯„ä¼°ç»“æœ:")
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ç²¾ç¡®ç‡: {precision:.4f}")
    print(f"å¬å›ç‡: {recall:.4f}")
    print(f"F1åˆ†æ•°: {f1:.4f}")
    print(f"AUC: {auc:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹ç±»åˆ«')
    plt.ylabel('çœŸå®ç±»åˆ«')
    plt.show()
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('å‡æ­£ä¾‹ç‡')
    plt.ylabel('çœŸæ­£ä¾‹ç‡')
    plt.title(f'{model_name} ROCæ›²çº¿')
    plt.legend()
    plt.show()
    
    # ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_prob)
    plt.figure(figsize=(8, 6))
    plt.plot(recall_curve, precision_curve)
    plt.xlabel('å¬å›ç‡')
    plt.ylabel('ç²¾ç¡®ç‡')
    plt.title(f'{model_name} ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
    plt.show()
    
    return model, accuracy, precision, recall, f1, auc

# è®­ç»ƒå’Œè¯„ä¼°å¤šä¸ªæ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(class_weight='balanced', probability=True, random_state=42)
}

results = []
for name, model in models.items():
    model_result = evaluate_model(model, X_train_smote, y_train_smote, X_test_processed, y_test, name)
    results.append((name,) + model_result[1:])

# æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½
results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC'])
print("\næ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
print(results_df)

# å¯è§†åŒ–æ¯”è¾ƒ
plt.figure(figsize=(12, 8))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']
for i, metric in enumerate(metrics):
    plt.subplot(2, 3, i+1)
    sns.barplot(x='Model', y=metric, data=results_df)
    plt.title(metric)
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

### 4. ç‰¹å¾é‡è¦æ€§åˆ†æ

äº†è§£å“ªäº›ç‰¹å¾å¯¹ä¿¡ç”¨é£é™©é¢„æµ‹æœ€é‡è¦ï¼š

```python
# ä½¿ç”¨éšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§
rf_model = models['Random Forest']
feature_names = numerical_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))

# è·å–ç‰¹å¾é‡è¦æ€§
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# å¯è§†åŒ–å‰15ä¸ªé‡è¦ç‰¹å¾
plt.figure(figsize=(12, 8))
plt.title('ç‰¹å¾é‡è¦æ€§')
plt.bar(range(15), importances[indices[:15]], align='center')
plt.xticks(range(15), [feature_names[i] for i in indices[:15]], rotation=90)
plt.tight_layout()
plt.show()

# ä½¿ç”¨SHAPå€¼è¿›è¡Œæ›´è¯¦ç»†çš„è§£é‡Š
import shap

# é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è¿›è¡Œè§£é‡Šï¼ˆä¾‹å¦‚ï¼Œéšæœºæ£®æ—ï¼‰
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_test_processed)

# å¯è§†åŒ–SHAPå€¼
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values[1], X_test_processed, feature_names=feature_names)
```

### 5. æ¨¡å‹ä¼˜åŒ–

ä½¿ç”¨äº¤å‰éªŒè¯å’Œç½‘æ ¼æœç´¢ä¼˜åŒ–æœ€ä½³æ¨¡å‹ï¼š

```python
from sklearn.model_selection import GridSearchCV

# å‡è®¾éšæœºæ£®æ—æ˜¯æœ€ä½³æ¨¡å‹
best_model = RandomForestClassifier(class_weight='balanced', random_state=42)

# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# ä½¿ç”¨ç½‘æ ¼æœç´¢æ‰¾åˆ°æœ€ä½³å‚æ•°
grid_search = GridSearchCV(
    estimator=best_model,
    param_grid=param_grid,
    cv=5,
    scoring='f1',  # ä½¿ç”¨F1åˆ†æ•°ä½œä¸ºä¼˜åŒ–ç›®æ ‡
    n_jobs=-1
)

grid_search.fit(X_train_smote, y_train_smote)

# æ‰“å°æœ€ä½³å‚æ•°
print("æœ€ä½³å‚æ•°:")
print(grid_search.best_params_)

# ä½¿ç”¨æœ€ä½³å‚æ•°çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
best_rf = grid_search.best_estimator_
evaluate_model(best_rf, X_train_smote, y_train_smote, X_test_processed, y_test, "ä¼˜åŒ–åçš„éšæœºæ£®æ—")
```

### 6. é˜ˆå€¼ä¼˜åŒ–

åœ¨ä¿¡ç”¨é£é™©è¯„ä¼°ä¸­ï¼Œé”™è¯¯åˆ†ç±»çš„æˆæœ¬æ˜¯ä¸å¯¹ç§°çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´åˆ†ç±»é˜ˆå€¼æ¥å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡ï¼š

```python
# è·å–é¢„æµ‹æ¦‚ç‡
y_prob = best_rf.predict_proba(X_test_processed)[:, 1]

# è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
thresholds = np.arange(0, 1, 0.01)
precision_scores = []
recall_scores = []
f1_scores = []

for threshold in thresholds:
    y_pred_threshold = (y_prob >= threshold).astype(int)
    precision_scores.append(precision_score(y_test, y_pred_threshold))
    recall_scores.append(recall_score(y_test, y_pred_threshold))
    f1_scores.append(f1_score(y_test, y_pred_threshold))

# å¯è§†åŒ–ä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precision_scores, label='ç²¾ç¡®ç‡')
plt.plot(thresholds, recall_scores, label='å¬å›ç‡')
plt.plot(thresholds, f1_scores, label='F1åˆ†æ•°')
plt.xlabel('é˜ˆå€¼')
plt.ylabel('åˆ†æ•°')
plt.title('ä¸åŒé˜ˆå€¼ä¸‹çš„æ¨¡å‹æ€§èƒ½')
plt.legend()
plt.grid(True)
plt.show()

# æ‰¾åˆ°F1åˆ†æ•°æœ€é«˜çš„é˜ˆå€¼
best_threshold = thresholds[np.argmax(f1_scores)]
print(f"æœ€ä½³é˜ˆå€¼: {best_threshold:.2f}")
print(f"åœ¨æœ€ä½³é˜ˆå€¼ä¸‹çš„F1åˆ†æ•°: {max(f1_scores):.4f}")

# ä½¿ç”¨æœ€ä½³é˜ˆå€¼è¿›è¡Œæœ€ç»ˆé¢„æµ‹
y_pred_final = (y_prob >= best_threshold).astype(int)
print("\nä½¿ç”¨æœ€ä½³é˜ˆå€¼çš„æœ€ç»ˆè¯„ä¼°ç»“æœ:")
print(classification_report(y_test, y_pred_final))
```

### 7. ä¸šåŠ¡è§£é‡Šä¸å»ºè®®

æœ€åï¼Œæˆ‘ä»¬éœ€è¦å°†æ¨¡å‹ç»“æœè½¬åŒ–ä¸ºä¸šåŠ¡æ´å¯Ÿï¼š

```python
# è®¡ç®—æ‹’ç»ç‡å’Œåè´¦ç‡
def calculate_business_metrics(y_true, y_pred, y_prob, threshold):
    # ä½¿ç”¨ç»™å®šé˜ˆå€¼çš„é¢„æµ‹
    y_pred_threshold = (y_prob >= threshold).astype(int)
    
    # è®¡ç®—æ‹’ç»ç‡ï¼ˆé¢„æµ‹ä¸ºåå®¢æˆ·çš„æ¯”ä¾‹ï¼‰
    rejection_rate = np.mean(y_pred_threshold)
    
    # è®¡ç®—åè´¦ç‡ï¼ˆåœ¨é¢„æµ‹ä¸ºå¥½å®¢æˆ·ä¸­å®é™…ä¸ºåå®¢æˆ·çš„æ¯”ä¾‹ï¼‰
    approved_indices = y_pred_threshold == 0
    if np.sum(approved_indices) > 0:
        bad_debt_rate = np.mean(y_true[approved_indices] == 1)
    else:
        bad_debt_rate = 0
    
    return rejection_rate, bad_debt_rate

# è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„ä¸šåŠ¡æŒ‡æ ‡
business_metrics = []
for threshold in thresholds:
    rejection_rate, bad_debt_rate = calculate_business_metrics(y_test, y_pred, y_prob, threshold)
    business_metrics.append((threshold, rejection_rate, bad_debt_rate))

business_df = pd.DataFrame(business_metrics, columns=['Threshold', 'Rejection Rate', 'Bad Debt Rate'])

# å¯è§†åŒ–ä¸šåŠ¡æŒ‡æ ‡
plt.figure(figsize=(10, 6))
plt.plot(business_df['Threshold'], business_df['Rejection Rate'], label='æ‹’ç»ç‡')
plt.plot(business_df['Threshold'], business_df['Bad Debt Rate'], label='åè´¦ç‡')
plt.xlabel('é˜ˆå€¼')
plt.ylabel('æ¯”ç‡')
plt.title('ä¸åŒé˜ˆå€¼ä¸‹çš„ä¸šåŠ¡æŒ‡æ ‡')
plt.legend()
plt.grid(True)
plt.show()

# æ ¹æ®ä¸šåŠ¡ç›®æ ‡é€‰æ‹©é˜ˆå€¼
# ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›åè´¦ç‡ä¸è¶…è¿‡5%
target_bad_debt = 0.05
valid_thresholds = business_df[business_df['Bad Debt Rate'] <= target_bad_debt]
if not valid_thresholds.empty:
    business_threshold = valid_thresholds.iloc[np.argmin(valid_thresholds['Rejection Rate'])]['Threshold']
    print(f"ä¸ºè¾¾åˆ°åè´¦ç‡ä¸è¶…è¿‡5%çš„ç›®æ ‡ï¼Œå»ºè®®ä½¿ç”¨é˜ˆå€¼: {business_threshold:.2f}")
    print(f"åœ¨æ­¤é˜ˆå€¼ä¸‹çš„æ‹’ç»ç‡: {valid_thresholds.iloc[np.argmin(valid_thresholds['Rejection Rate'])]['Rejection Rate']:.2f}")
    print(f"åœ¨æ­¤é˜ˆå€¼ä¸‹çš„åè´¦ç‡: {valid_thresholds.iloc[np.argmin(valid_thresholds['Rejection Rate'])]['Bad Debt Rate']:.2f}")
else:
    print("æ— æ³•æ‰¾åˆ°æ»¡è¶³åè´¦ç‡ç›®æ ‡çš„é˜ˆå€¼")
```

## è¿›é˜¶æŒ‘æˆ˜

å¦‚æœä½ å·²ç»å®Œæˆäº†åŸºæœ¬ä»»åŠ¡ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹è¿›é˜¶æŒ‘æˆ˜ï¼š

1. **æˆæœ¬æ•æ„Ÿå­¦ä¹ **ï¼šè€ƒè™‘ä¸åŒç±»å‹é”™è¯¯çš„ä¸šåŠ¡æˆæœ¬ï¼Œæ„å»ºæˆæœ¬æ•æ„Ÿçš„æ¨¡å‹
2. **ç‰¹å¾å·¥ç¨‹è¿›é˜¶**ï¼šåˆ›å»ºäº¤äº’ç‰¹å¾ã€å¤šé¡¹å¼ç‰¹å¾æˆ–åŸºäºé¢†åŸŸçŸ¥è¯†çš„ç‰¹å¾
3. **æ¨¡å‹è§£é‡Šè¿›é˜¶**ï¼šä½¿ç”¨LIMEæˆ–SHAPç­‰å·¥å…·æä¾›ä¸ªä½“é¢„æµ‹çš„è§£é‡Š
4. **å…¬å¹³æ€§åˆ†æ**ï¼šè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒäººå£ç»Ÿè®¡ç¾¤ä½“ä¸Šçš„è¡¨ç°ï¼Œæ£€æµ‹å’Œå‡è½»æ½œåœ¨åè§
5. **éƒ¨ç½²è€ƒé‡**ï¼šè®¾è®¡æ¨¡å‹ç›‘æ§å’Œæ›´æ–°ç­–ç•¥ï¼Œå¤„ç†æ¦‚å¿µæ¼‚ç§»é—®é¢˜

## å°ç»“ä¸åæ€

é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ„å»ºä¿¡ç”¨é£é™©è¯„ä¼°æ¨¡å‹ï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶å°†æ¨¡å‹ç»“æœè½¬åŒ–ä¸ºä¸šåŠ¡å†³ç­–ã€‚ä¿¡ç”¨é£é™©è¯„ä¼°æ˜¯ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦å¹³è¡¡å¤šç§å› ç´ ï¼ŒåŒ…æ‹¬é¢„æµ‹å‡†ç¡®æ€§ã€ä¸šåŠ¡ç›®æ ‡å’Œä¼¦ç†è€ƒé‡ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¿¡ç”¨é£é™©æ¨¡å‹éœ€è¦å®šæœŸæ›´æ–°å’Œç›‘æ§ï¼Œä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„ç»æµç¯å¢ƒå’Œå®¢æˆ·è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å…¬å¹³æ€§å’Œé€æ˜åº¦ä¹Ÿæ˜¯é‡è¦çš„è€ƒé‡å› ç´ ï¼Œç¡®ä¿ä¿¡è´·å†³ç­–ä¸ä¼šå¯¹ç‰¹å®šç¾¤ä½“é€ æˆä¸å…¬å¹³çš„å½±å“ã€‚

### æ€è€ƒé—®é¢˜

1. åœ¨ä¿¡ç”¨é£é™©è¯„ä¼°ä¸­ï¼Œå¦‚ä½•å¹³è¡¡æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§éœ€æ±‚ï¼Ÿ
2. ä¸åŒçš„é‡‡æ ·æ–¹æ³•å¦‚ä½•å½±å“æ¨¡å‹å¯¹å°‘æ•°ç±»ï¼ˆé«˜é£é™©å®¢æˆ·ï¼‰çš„é¢„æµ‹èƒ½åŠ›ï¼Ÿ
3. å¦‚ä½•å°†ä¿¡ç”¨é£é™©æ¨¡å‹ä¸ä¸šåŠ¡æµç¨‹é›†æˆï¼Œä½¿å…¶æˆä¸ºæœ‰æ•ˆçš„å†³ç­–æ”¯æŒå·¥å…·ï¼Ÿ

<div class="practice-link">
  <a href="/projects/clustering/customer-segmentation.html" class="button">ä¸‹ä¸€ä¸ªæ¨¡å—ï¼šèšç±»åˆ†æé¡¹ç›®</a>
</div> 