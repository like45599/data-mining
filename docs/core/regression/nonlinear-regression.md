# éçº¿æ€§å›å½’æ–¹æ³•

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">ğŸ“š</span>æœ¬èŠ‚è¦ç‚¹
  </div>
  <div class="knowledge-card__content">
    <ul>
      <li>ç†è§£éçº¿æ€§å›å½’çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨åœºæ™¯</li>
      <li>æŒæ¡å¤šé¡¹å¼å›å½’ã€å†³ç­–æ ‘å›å½’ç­‰å¸¸ç”¨éçº¿æ€§å›å½’æ–¹æ³•</li>
      <li>å­¦ä¹ å¦‚ä½•é€‰æ‹©åˆé€‚çš„éçº¿æ€§å›å½’æ¨¡å‹</li>
      <li>äº†è§£éçº¿æ€§å›å½’ä¸­çš„è¿‡æ‹Ÿåˆé—®é¢˜åŠè§£å†³æ–¹æ¡ˆ</li>
    </ul>
  </div>
</div>

## éçº¿æ€§å›å½’æ¦‚è¿°

éçº¿æ€§å›å½’æ˜¯ä¸€ç§ç”¨äºå»ºç«‹è‡ªå˜é‡ä¸å› å˜é‡ä¹‹é—´éçº¿æ€§å…³ç³»çš„ç»Ÿè®¡æ–¹æ³•ã€‚å½“æ•°æ®å‘ˆç°æ˜æ˜¾çš„éçº¿æ€§ç‰¹å¾æ—¶ï¼Œçº¿æ€§å›å½’æ¨¡å‹å¾€å¾€æ— æ³•å‡†ç¡®æ•æ‰æ•°æ®çš„çœŸå®å…³ç³»ï¼Œæ­¤æ—¶éœ€è¦ä½¿ç”¨éçº¿æ€§å›å½’æ¨¡å‹ã€‚

### åº”ç”¨åœºæ™¯

éçº¿æ€§å›å½’åœ¨ä»¥ä¸‹åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼š

- **ç”Ÿç‰©å­¦ç”Ÿé•¿æ›²çº¿**ï¼šå¦‚ç»†èŒç”Ÿé•¿ã€ç§ç¾¤åŠ¨æ€ç­‰éµå¾ªæŒ‡æ•°æˆ–Så½¢æ›²çº¿
- **ç‰©ç†ç°è±¡**ï¼šå¦‚æ”¾å°„æ€§è¡°å˜ã€çƒ­åŠ›å­¦è¿‡ç¨‹ç­‰
- **ç»æµå­¦**ï¼šå¦‚æ¶ˆè´¹è€…è¡Œä¸ºã€å¸‚åœºé¥±å’Œåº¦åˆ†æ
- **è¯ç‰©ååº”**ï¼šè¯ç‰©å‰‚é‡ä¸æ•ˆæœçš„éçº¿æ€§å…³ç³»

### ä¸çº¿æ€§å›å½’çš„åŒºåˆ«

| ç‰¹æ€§ | çº¿æ€§å›å½’ | éçº¿æ€§å›å½’ |
|------|----------|------------|
| æ¨¡å‹å½¢å¼ | $y = w_0 + w_1x_1 + ... + w_nx_n$ | å¯ä»¥æ˜¯ä»»ä½•å‡½æ•°å½¢å¼ï¼Œå¦‚æŒ‡æ•°ã€å¯¹æ•°ã€å¤šé¡¹å¼ç­‰ |
| å‚æ•°ä¼°è®¡ | é€šå¸¸æœ‰è§£æè§£ï¼ˆæœ€å°äºŒä¹˜æ³•ï¼‰ | é€šå¸¸éœ€è¦è¿­ä»£ä¼˜åŒ–ç®—æ³• |
| è§£é‡Šæ€§ | è¾ƒå¼ºï¼Œç³»æ•°ç›´æ¥è¡¨ç¤ºå˜é‡å½±å“ | å¯èƒ½è¾ƒå¼±ï¼Œå–å†³äºå…·ä½“æ¨¡å‹ |
| è®¡ç®—å¤æ‚åº¦ | è¾ƒä½ | è¾ƒé«˜ |
| è¿‡æ‹Ÿåˆé£é™© | è¾ƒä½ | è¾ƒé«˜ |

## å¸¸ç”¨éçº¿æ€§å›å½’æ–¹æ³•

### 1. å¤šé¡¹å¼å›å½’

å¤šé¡¹å¼å›å½’æ˜¯çº¿æ€§å›å½’çš„æ‰©å±•ï¼Œé€šè¿‡æ·»åŠ è‡ªå˜é‡çš„é«˜æ¬¡é¡¹æ¥æ•æ‰éçº¿æ€§å…³ç³»ã€‚

**æ•°å­¦è¡¨è¾¾å¼**ï¼š$y = w_0 + w_1x + w_2x^2 + ... + w_nx^n$

**ä¼˜ç‚¹**ï¼š
- å®ç°ç®€å•ï¼Œå¯ä»¥ä½¿ç”¨çº¿æ€§å›å½’çš„æ¡†æ¶
- å¯¹äºä¸­ç­‰å¤æ‚åº¦çš„éçº¿æ€§å…³ç³»æ•ˆæœå¥½

**ç¼ºç‚¹**ï¼š
- é«˜æ¬¡å¤šé¡¹å¼å®¹æ˜“è¿‡æ‹Ÿåˆ
- å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# ç”Ÿæˆéçº¿æ€§æ•°æ®
np.random.seed(0)
X = np.sort(np.random.rand(100, 1), axis=0)
y = np.sin(2 * np.pi * X).ravel() + np.random.randn(100) * 0.1

# åˆ›å»ºå¤šé¡¹å¼å›å½’æ¨¡å‹
degrees = [1, 3, 5, 9]
plt.figure(figsize=(14, 5))

for i, degree in enumerate(degrees):
    ax = plt.subplot(1, len(degrees), i + 1)
    plt.setp(ax, xticks=(), yticks=())
    
    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)
    linear_regression = LinearRegression()
    pipeline = Pipeline([
        ("polynomial_features", polynomial_features),
        ("linear_regression", linear_regression)
    ])
    pipeline.fit(X, y)
    
    # é¢„æµ‹
    X_test = np.linspace(0, 1, 100)[:, np.newaxis]
    plt.plot(X_test, pipeline.predict(X_test), label=f"åº¦æ•° {degree}")
    plt.scatter(X, y, color='navy', s=30, marker='o', label="è®­ç»ƒç‚¹")
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend(loc="best")
    plt.title(f"{degree}æ¬¡å¤šé¡¹å¼")

plt.tight_layout()
plt.show()
```

  </div>
</div>

### 2. å†³ç­–æ ‘å›å½’

å†³ç­–æ ‘å›å½’é€šè¿‡å°†ç‰¹å¾ç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªåŒºåŸŸï¼Œå¹¶åœ¨æ¯ä¸ªåŒºåŸŸå†…ä½¿ç”¨å¸¸æ•°å€¼è¿›è¡Œé¢„æµ‹ã€‚

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥æ•æ‰å¤æ‚çš„éçº¿æ€§å…³ç³»
- ä¸éœ€è¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
- å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ

**ç¼ºç‚¹**ï¼š
- å®¹æ˜“è¿‡æ‹Ÿåˆ
- ä¸èƒ½å¤–æ¨åˆ°è®­ç»ƒæ•°æ®èŒƒå›´ä¹‹å¤–

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.tree import DecisionTreeRegressor
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel() + np.sin(6 * X).ravel() + np.random.randn(80) * 0.1

# æ‹Ÿåˆå†³ç­–æ ‘
regr = DecisionTreeRegressor(max_depth=5)
regr.fit(X, y)

# é¢„æµ‹
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_pred = regr.predict(X_test)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X, y, s=20, color="darkorange", label="æ•°æ®ç‚¹")
plt.plot(X_test, y_pred, color="cornflowerblue", label="å†³ç­–æ ‘é¢„æµ‹", linewidth=2)
plt.xlabel("ç‰¹å¾")
plt.ylabel("ç›®æ ‡")
plt.title("å†³ç­–æ ‘å›å½’")
plt.legend()
plt.show()
```

  </div>
</div>

### 3. æ”¯æŒå‘é‡å›å½’(SVR)

SVRæ˜¯æ”¯æŒå‘é‡æœºåœ¨å›å½’é—®é¢˜ä¸Šçš„åº”ç”¨ï¼Œé€šè¿‡å…è®¸ä¸€å®šçš„è¯¯å·®ï¼Œå¯»æ‰¾æœ€å¤§é—´éš”è¶…å¹³é¢ã€‚

**ä¼˜ç‚¹**ï¼š
- å¯¹é«˜ç»´æ•°æ®æœ‰æ•ˆ
- é€šè¿‡æ ¸æŠ€å·§å¯ä»¥å¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³»
- å¯¹å¼‚å¸¸å€¼è¾ƒä¸ºé²æ£’

**ç¼ºç‚¹**ï¼š
- è®¡ç®—å¤æ‚åº¦é«˜
- å‚æ•°è°ƒä¼˜å›°éš¾
- ä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®é›†

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.svm import SVR
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
np.random.seed(0)
X = np.sort(5 * np.random.rand(100, 1), axis=0)
y = np.sin(X).ravel() + np.random.randn(100) * 0.1

# åˆ›å»ºSVRæ¨¡å‹
svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)
svr_lin = SVR(kernel='linear', C=100, gamma='auto')
svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=0.1, coef0=1)

# æ‹Ÿåˆæ¨¡å‹
svrs = [svr_rbf, svr_lin, svr_poly]
kernel_label = ['RBF', 'Linear', 'Polynomial']
model_color = ['m', 'c', 'g']

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4), sharey=True)
for ix, svr in enumerate(svrs):
    axes[ix].plot(X, svr.fit(X, y).predict(X), color=model_color[ix], lw=2,
                 label='{} æ¨¡å‹'.format(kernel_label[ix]))
    axes[ix].scatter(X, y, color='darkorange', label='æ•°æ®ç‚¹')
    axes[ix].set_xlabel('ç‰¹å¾')
    axes[ix].set_ylabel('ç›®æ ‡')
    axes[ix].set_title('{} æ ¸'.format(kernel_label[ix]))
    axes[ix].legend()
fig.tight_layout()
plt.show()
```

  </div>
</div>

### 4. éšæœºæ£®æ—å›å½’

éšæœºæ£®æ—å›å½’æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¤šä¸ªå†³ç­–æ ‘å¹¶å–å¹³å‡å€¼æ¥æé«˜é¢„æµ‹æ€§èƒ½å’Œå‡å°‘è¿‡æ‹Ÿåˆã€‚

**ä¼˜ç‚¹**ï¼š
- é¢„æµ‹ç²¾åº¦é«˜
- ä¸æ˜“è¿‡æ‹Ÿåˆ
- å¯ä»¥å¤„ç†é«˜ç»´æ•°æ®
- å¯ä»¥è¯„ä¼°ç‰¹å¾é‡è¦æ€§

**ç¼ºç‚¹**ï¼š
- è®¡ç®—å¤æ‚åº¦è¾ƒé«˜
- æ¨¡å‹è§£é‡Šæ€§è¾ƒå·®
- å¯¹æç«¯å€¼çš„é¢„æµ‹å¯èƒ½ä¸å‡†ç¡®

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.ensemble import RandomForestRegressor
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
np.random.seed(0)
X = np.sort(5 * np.random.rand(100, 1), axis=0)
y = np.sin(X).ravel() + np.sin(6 * X).ravel() + np.random.randn(100) * 0.1

# åˆ›å»ºéšæœºæ£®æ—å›å½’æ¨¡å‹
rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
rf.fit(X, y)

# é¢„æµ‹
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_pred = rf.predict(X_test)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='darkorange', label='æ•°æ®ç‚¹')
plt.plot(X_test, y_pred, color='navy', label='éšæœºæ£®æ—é¢„æµ‹')
plt.xlabel('ç‰¹å¾')
plt.ylabel('ç›®æ ‡')
plt.title('éšæœºæ£®æ—å›å½’')
plt.legend()
plt.show()
```

  </div>
</div>

### 5. ç¥ç»ç½‘ç»œå›å½’

ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥å»ºæ¨¡æå…¶å¤æ‚çš„éçº¿æ€§å…³ç³»
- é€‚åˆå¤§è§„æ¨¡æ•°æ®
- å¯ä»¥è‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤º

**ç¼ºç‚¹**ï¼š
- éœ€è¦å¤§é‡æ•°æ®
- è®¡ç®—èµ„æºéœ€æ±‚é«˜
- å‚æ•°è°ƒä¼˜å›°éš¾
- è§£é‡Šæ€§å·®

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.neural_network import MLPRegressor
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
np.random.seed(0)
X = np.sort(5 * np.random.rand(100, 1), axis=0)
y = np.sin(X).ravel() + np.sin(6 * X).ravel() + np.random.randn(100) * 0.1

# åˆ›å»ºç¥ç»ç½‘ç»œå›å½’æ¨¡å‹
nn_reg = MLPRegressor(
    hidden_layer_sizes=(100, 50),
    activation='relu',
    solver='adam',
    alpha=0.0001,
    batch_size='auto',
    learning_rate='adaptive',
    max_iter=2000,
    random_state=42
)

# æ‹Ÿåˆæ¨¡å‹
nn_reg.fit(X, y)

# é¢„æµ‹
X_test = np.linspace(0, 5, 500).reshape(-1, 1)
y_pred = nn_reg.predict(X_test)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='darkorange', label='æ•°æ®ç‚¹')
plt.plot(X_test, y_pred, color='navy', label='ç¥ç»ç½‘ç»œé¢„æµ‹')
plt.xlabel('ç‰¹å¾')
plt.ylabel('ç›®æ ‡')
plt.title('ç¥ç»ç½‘ç»œå›å½’')
plt.legend()
plt.show()
```

  </div>
</div>

## æ¨¡å‹é€‰æ‹©ä¸è¿‡æ‹Ÿåˆå¤„ç†

### å¦‚ä½•é€‰æ‹©åˆé€‚çš„éçº¿æ€§å›å½’æ¨¡å‹

é€‰æ‹©åˆé€‚çš„éçº¿æ€§å›å½’æ¨¡å‹éœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š

1. **æ•°æ®ç‰¹æ€§**ï¼šäº†è§£æ•°æ®çš„åˆ†å¸ƒå’Œæ½œåœ¨çš„éçº¿æ€§å…³ç³»ç±»å‹
2. **æ ·æœ¬é‡**ï¼šå¤æ‚æ¨¡å‹éœ€è¦æ›´å¤šæ•°æ®æ”¯æŒ
3. **è®¡ç®—èµ„æº**ï¼šæŸäº›æ¨¡å‹ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰éœ€è¦æ›´å¤šè®¡ç®—èµ„æº
4. **è§£é‡Šæ€§éœ€æ±‚**ï¼šå¦‚æœéœ€è¦è§£é‡Šæ¨¡å‹ï¼Œå¤šé¡¹å¼å›å½’å¯èƒ½æ›´åˆé€‚
5. **é¢„æµ‹ç²¾åº¦è¦æ±‚**ï¼šé€šå¸¸å¤æ‚æ¨¡å‹å¯ä»¥æä¾›æ›´é«˜çš„é¢„æµ‹ç²¾åº¦

### å¤„ç†è¿‡æ‹Ÿåˆçš„æ–¹æ³•

éçº¿æ€§å›å½’æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„å¤„ç†æ–¹æ³•ï¼š

1. **æ­£åˆ™åŒ–**ï¼š
   - L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰ï¼šä¿ƒä½¿éƒ¨åˆ†ç‰¹å¾ç³»æ•°ä¸ºé›¶
   - L2æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰ï¼šå‡å°æ‰€æœ‰ç‰¹å¾ç³»æ•°çš„å¤§å°

2. **äº¤å‰éªŒè¯**ï¼šä½¿ç”¨kæŠ˜äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³æ¨¡å‹å¤æ‚åº¦

3. **ç‰¹å¾é€‰æ‹©**ï¼šå‡å°‘ä¸ç›¸å…³ç‰¹å¾ï¼Œé™ä½æ¨¡å‹å¤æ‚åº¦

4. **æ—©åœæ³•**ï¼šåœ¨è¿­ä»£ç®—æ³•ä¸­ï¼Œå½“éªŒè¯é›†æ€§èƒ½å¼€å§‹ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ

5. **é›†æˆæ–¹æ³•**ï¼šç»“åˆå¤šä¸ªç®€å•æ¨¡å‹çš„é¢„æµ‹ç»“æœ

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹ï¼šæ­£åˆ™åŒ–å¤šé¡¹å¼å›å½’</div>
  <div class="code-example__content">

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline

# ç”Ÿæˆæ•°æ®
np.random.seed(0)
n_samples = 30
X = np.sort(np.random.rand(n_samples))
y = np.sin(2 * np.pi * X) + np.random.randn(n_samples) * 0.1
X = X.reshape(-1, 1)

# åˆ›å»ºå¤šé¡¹å¼å›å½’æ¨¡å‹ï¼Œä½¿ç”¨Ridgeæ­£åˆ™åŒ–
degrees = [1, 4, 15]
alphas = [0, 0.001, 1.0]

plt.figure(figsize=(14, 8))
for i, degree in enumerate(degrees):
    for j, alpha in enumerate(alphas):
        ax = plt.subplot(len(degrees), len(alphas), i * len(alphas) + j + 1)
        
        model = Pipeline([
            ('poly', PolynomialFeatures(degree=degree, include_bias=False)),
            ('ridge', Ridge(alpha=alpha))
        ])
        
        model.fit(X, y)
        
        # é¢„æµ‹
        X_test = np.linspace(0, 1, 100).reshape(-1, 1)
        plt.plot(X_test, model.predict(X_test), label=f"Model")
        plt.scatter(X, y, color='navy', s=30, marker='o', label="Training points")
        plt.xlabel("x")
        plt.ylabel("y")
        plt.xlim((0, 1))
        plt.ylim((-2, 2))
        plt.title(f"degree={degree}, alpha={alpha}")
        
plt.tight_layout()
plt.show()
```

  </div>
</div>

## å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹ï¼šé¢„æµ‹æˆ¿ä»·

åœ¨æˆ¿åœ°äº§é¢†åŸŸï¼Œæˆ¿ä»·ä¸å¤šä¸ªå› ç´ ï¼ˆå¦‚é¢ç§¯ã€ä½ç½®ã€æˆ¿é¾„ç­‰ï¼‰ä¹‹é—´é€šå¸¸å­˜åœ¨éçº¿æ€§å…³ç³»ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨éçº¿æ€§å›å½’é¢„æµ‹æˆ¿ä»·çš„ç¤ºä¾‹ï¼š

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹ï¼šæˆ¿ä»·é¢„æµ‹</div>
  <div class="code-example__content">

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# åŠ è½½åŠ å·æˆ¿ä»·æ•°æ®é›†
housing = fetch_california_housing()
X, y = housing.data, housing.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# åˆ›å»ºæ¢¯åº¦æå‡å›å½’æ¨¡å‹
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)

# è®­ç»ƒæ¨¡å‹
gbr.fit(X_train_scaled, y_train)

# é¢„æµ‹
y_pred = gbr.predict(X_test_scaled)

# è¯„ä¼°æ¨¡å‹
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
print(f"å†³å®šç³»æ•° (RÂ²): {r2:.4f}")

# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
feature_importance = gbr.feature_importances_
sorted_idx = np.argsort(feature_importance)
plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), np.array(housing.feature_names)[sorted_idx])
plt.title('ç‰¹å¾é‡è¦æ€§')
plt.tight_layout()
plt.show()

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('å®é™…å€¼')
plt.ylabel('é¢„æµ‹å€¼')
plt.title('å®é™…å€¼ vs é¢„æµ‹å€¼')
plt.tight_layout()
plt.show()
```

  </div>
</div>

## æ€»ç»“

éçº¿æ€§å›å½’æ˜¯å¤„ç†å¤æ‚æ•°æ®å…³ç³»çš„å¼ºå¤§å·¥å…·ï¼Œä½†é€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œé¿å…è¿‡æ‹Ÿåˆæ˜¯å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡ç†è§£ä¸åŒæ¨¡å‹çš„ç‰¹æ€§ã€é€‚å½“çš„æ­£åˆ™åŒ–å’Œäº¤å‰éªŒè¯ï¼Œå¯ä»¥æ„å»ºæ—¢å‡†ç¡®åˆç¨³å¥çš„éçº¿æ€§å›å½’æ¨¡å‹ã€‚

<div class="practice-link">
  <a href="/projects/prediction.html" class="button">å‰å¾€å®è·µé¡¹ç›®</a>
</div>
