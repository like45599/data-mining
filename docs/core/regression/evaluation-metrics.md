# å›å½’è¯„ä¼°æŒ‡æ ‡

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">ğŸ“š</span>æœ¬èŠ‚è¦ç‚¹
  </div>
  <div class="knowledge-card__content">
    <ul>
      <li>ç†è§£å¸¸ç”¨å›å½’è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—æ–¹æ³•å’Œæ„ä¹‰</li>
      <li>æŒæ¡ä¸åŒè¯„ä¼°æŒ‡æ ‡çš„é€‚ç”¨åœºæ™¯å’Œå±€é™æ€§</li>
      <li>å­¦ä¹ å¦‚ä½•ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°å›å½’æ¨¡å‹</li>
      <li>äº†è§£å¦‚ä½•é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡è¿›è¡Œæ¨¡å‹æ¯”è¾ƒ</li>
    </ul>
  </div>
</div>

## å¸¸ç”¨å›å½’è¯„ä¼°æŒ‡æ ‡

è¯„ä¼°å›å½’æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡æœ‰å¤šç§ï¼Œæ¯ç§æŒ‡æ ‡éƒ½æœ‰å…¶ç‰¹å®šçš„ç”¨é€”å’Œè§£é‡Šã€‚

### å‡æ–¹è¯¯å·®(MSE)

å‡æ–¹è¯¯å·®æ˜¯æœ€å¸¸ç”¨çš„å›å½’è¯„ä¼°æŒ‡æ ‡ä¹‹ä¸€ï¼Œè®¡ç®—é¢„æµ‹å€¼ä¸å®é™…å€¼å·®å¼‚çš„å¹³æ–¹çš„å¹³å‡å€¼ï¼š

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

å…¶ä¸­ï¼š
- $y_i$ æ˜¯å®é™…å€¼
- $\hat{y}_i$ æ˜¯é¢„æµ‹å€¼
- $n$ æ˜¯æ ·æœ¬æ•°é‡

MSEå¯¹è¾ƒå¤§çš„è¯¯å·®ç»™äºˆæ›´é«˜çš„æƒ©ç½šï¼Œä½†å•ä½æ˜¯ç›®æ ‡å˜é‡çš„å¹³æ–¹ã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.metrics import mean_squared_error

# è®¡ç®—MSE
mse = mean_squared_error(y_true, y_pred)
print(f"å‡æ–¹è¯¯å·®(MSE): {mse:.4f}")
```

  </div>
</div>

### å‡æ–¹æ ¹è¯¯å·®(RMSE)

å‡æ–¹æ ¹è¯¯å·®æ˜¯MSEçš„å¹³æ–¹æ ¹ï¼Œä½¿å¾—å•ä½ä¸ç›®æ ‡å˜é‡ç›¸åŒï¼Œæ›´æ˜“äºè§£é‡Šï¼š

$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
import numpy as np
from sklearn.metrics import mean_squared_error

# è®¡ç®—RMSE
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print(f"å‡æ–¹æ ¹è¯¯å·®(RMSE): {rmse:.4f}")
```

  </div>
</div>

### å¹³å‡ç»å¯¹è¯¯å·®(MAE)

å¹³å‡ç»å¯¹è¯¯å·®è®¡ç®—é¢„æµ‹å€¼ä¸å®é™…å€¼å·®å¼‚çš„ç»å¯¹å€¼çš„å¹³å‡å€¼ï¼š

$$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$

MAEå¯¹å¼‚å¸¸å€¼ä¸å¦‚MSEæ•æ„Ÿï¼Œä¸”æ˜“äºè§£é‡Šã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.metrics import mean_absolute_error

# è®¡ç®—MAE
mae = mean_absolute_error(y_true, y_pred)
print(f"å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae:.4f}")
```

  </div>
</div>

### å†³å®šç³»æ•°(RÂ²)

å†³å®šç³»æ•°è¡¡é‡æ¨¡å‹è§£é‡Šçš„ç›®æ ‡å˜é‡æ–¹å·®æ¯”ä¾‹ï¼ŒèŒƒå›´é€šå¸¸åœ¨0åˆ°1ä¹‹é—´ï¼ˆå¯ä»¥ä¸ºè´Ÿï¼‰ï¼š

$$R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$

å…¶ä¸­ï¼š
- $\bar{y}$ æ˜¯å®é™…å€¼çš„å¹³å‡å€¼

RÂ²å€¼ä¸º1è¡¨ç¤ºå®Œç¾æ‹Ÿåˆï¼Œ0è¡¨ç¤ºæ¨¡å‹ä¸æ¯”ç®€å•åœ°é¢„æµ‹å¹³å‡å€¼å¥½ã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.metrics import r2_score

# è®¡ç®—RÂ²
r2 = r2_score(y_true, y_pred)
print(f"å†³å®šç³»æ•°(RÂ²): {r2:.4f}")
```

  </div>
</div>

### è°ƒæ•´RÂ²

è°ƒæ•´RÂ²è€ƒè™‘äº†ç‰¹å¾æ•°é‡ï¼Œå¯¹æ·»åŠ ä¸ç›¸å…³ç‰¹å¾è¿›è¡Œæƒ©ç½šï¼š

$$\text{Adjusted } R^2 = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}$$

å…¶ä¸­ï¼š
- $n$ æ˜¯æ ·æœ¬æ•°é‡
- $p$ æ˜¯ç‰¹å¾æ•°é‡

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
def adjusted_r2_score(y_true, y_pred, n_features):
    r2 = r2_score(y_true, y_pred)
    n = len(y_true)
    p = n_features
    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    return adjusted_r2

# è®¡ç®—è°ƒæ•´RÂ²
adj_r2 = adjusted_r2_score(y_true, y_pred, X.shape[1])
print(f"è°ƒæ•´å†³å®šç³»æ•°(Adjusted RÂ²): {adj_r2:.4f}")
```

  </div>
</div>

### å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®(MAPE)

å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®è®¡ç®—é¢„æµ‹å€¼ä¸å®é™…å€¼å·®å¼‚çš„ç™¾åˆ†æ¯”çš„å¹³å‡å€¼ï¼š

$$MAPE = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|$$

MAPEæä¾›äº†ç›¸å¯¹è¯¯å·®çš„åº¦é‡ï¼Œä½†åœ¨å®é™…å€¼æ¥è¿‘é›¶æ—¶å¯èƒ½å‡ºç°é—®é¢˜ã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    # é¿å…é™¤ä»¥é›¶
    mask = y_true != 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100

# è®¡ç®—MAPE
mape = mean_absolute_percentage_error(y_true, y_pred)
print(f"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®(MAPE): {mape:.4f}%")
```

  </div>
</div>

### ä¸­ä½æ•°ç»å¯¹è¯¯å·®(MedAE)

ä¸­ä½æ•°ç»å¯¹è¯¯å·®è®¡ç®—é¢„æµ‹å€¼ä¸å®é™…å€¼å·®å¼‚çš„ç»å¯¹å€¼çš„ä¸­ä½æ•°ï¼š

$$MedAE = \text{median}(|y_1 - \hat{y}_1|, |y_2 - \hat{y}_2|, ..., |y_n - \hat{y}_n|)$$

MedAEå¯¹å¼‚å¸¸å€¼æ›´åŠ é²æ£’ã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.metrics import median_absolute_error

# è®¡ç®—MedAE
medae = median_absolute_error(y_true, y_pred)
print(f"ä¸­ä½æ•°ç»å¯¹è¯¯å·®(MedAE): {medae:.4f}")
```

  </div>
</div>

## è¯„ä¼°æŒ‡æ ‡çš„é€‰æ‹©

ä¸åŒçš„è¯„ä¼°æŒ‡æ ‡é€‚ç”¨äºä¸åŒçš„åœºæ™¯ï¼š

1. **MSE/RMSE**ï¼šå½“è¾ƒå¤§è¯¯å·®éœ€è¦æ›´é«˜æƒ©ç½šæ—¶ä½¿ç”¨ï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
2. **MAE**ï¼šå½“æ‰€æœ‰è¯¯å·®åº”è¢«å¹³ç­‰å¯¹å¾…æ—¶ä½¿ç”¨ï¼Œå¯¹å¼‚å¸¸å€¼è¾ƒä¸æ•æ„Ÿ
3. **RÂ²**ï¼šå½“éœ€è¦æ¯”è¾ƒä¸åŒå°ºåº¦çš„ç›®æ ‡å˜é‡æ—¶ä½¿ç”¨
4. **è°ƒæ•´RÂ²**ï¼šå½“æ¯”è¾ƒå…·æœ‰ä¸åŒç‰¹å¾æ•°é‡çš„æ¨¡å‹æ—¶ä½¿ç”¨
5. **MAPE**ï¼šå½“ç›¸å¯¹è¯¯å·®æ›´é‡è¦æ—¶ä½¿ç”¨ï¼Œä½†ç›®æ ‡å˜é‡ä¸åº”æ¥è¿‘é›¶
6. **MedAE**ï¼šå½“æ•°æ®ä¸­å­˜åœ¨å¼‚å¸¸å€¼æ—¶ä½¿ç”¨


## ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹

äº¤å‰éªŒè¯é€šè¿‡åœ¨ä¸åŒçš„æ•°æ®å­é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œæä¾›æ›´å¯é çš„æ€§èƒ½ä¼°è®¡ã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.linear_model import LinearRegression

# åˆ›å»ºæ¨¡å‹
model = LinearRegression()

# åˆ›å»ºKæŠ˜äº¤å‰éªŒè¯
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# è®¡ç®—äº¤å‰éªŒè¯åˆ†æ•°
mse_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')
r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')

# æ˜¾ç¤ºç»“æœ
print("MSEäº¤å‰éªŒè¯åˆ†æ•°:")
for i, mse in enumerate(mse_scores):
    print(f"æŠ˜{i+1}: {mse:.4f}")
print(f"å¹³å‡MSE: {mse_scores.mean():.4f}")
print(f"æ ‡å‡†å·®: {mse_scores.std():.4f}")

print("\nRÂ²äº¤å‰éªŒè¯åˆ†æ•°:")
for i, r2 in enumerate(r2_scores):
    print(f"æŠ˜{i+1}: {r2:.4f}")
print(f"å¹³å‡RÂ²: {r2_scores.mean():.4f}")
print(f"æ ‡å‡†å·®: {r2_scores.std():.4f}")
```

  </div>
</div>

### å­¦ä¹ æ›²çº¿

å­¦ä¹ æ›²çº¿æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½å¦‚ä½•éšè®­ç»ƒé›†å¤§å°å˜åŒ–ï¼Œæœ‰åŠ©äºè¯Šæ–­è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

# è®¡ç®—å­¦ä¹ æ›²çº¿
train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, scoring='neg_mean_squared_error',
    train_sizes=np.linspace(0.1, 1.0, 10), random_state=42
)

# è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
train_mean = -train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
test_mean = -test_scores.mean(axis=1)
test_std = test_scores.std(axis=1)

# ç»˜åˆ¶å­¦ä¹ æ›²çº¿
plt.figure(figsize=(10, 6))
plt.grid()
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="g")
plt.plot(train_sizes, train_mean, 'o-', color="r", label="è®­ç»ƒé›†MSE")
plt.plot(train_sizes, test_mean, 'o-', color="g", label="éªŒè¯é›†MSE")
plt.xlabel("è®­ç»ƒæ ·æœ¬æ•°")
plt.ylabel("MSE")
plt.title("å­¦ä¹ æ›²çº¿")
plt.legend(loc="best")
plt.show()
```

  </div>
</div>

## æ¯”è¾ƒå¤šä¸ªæ¨¡å‹

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸éœ€è¦æ¯”è¾ƒå¤šä¸ªå›å½’æ¨¡å‹çš„æ€§èƒ½ã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import cross_val_score

# åˆ›å»ºæ¨¡å‹
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'SVR': SVR(),
    'Neural Network': MLPRegressor(random_state=42)
}

# è¯„ä¼°æŒ‡æ ‡
metrics = {
    'MSE': 'neg_mean_squared_error',
    'MAE': 'neg_mean_absolute_error',
    'RÂ²': 'r2'
}

# è®¡ç®—äº¤å‰éªŒè¯åˆ†æ•°
results = {}
for name, model in models.items():
    model_results = {}
    for metric_name, metric in metrics.items():
        scores = cross_val_score(model, X, y, cv=5, scoring=metric)
        if metric.startswith('neg_'):
            scores = -scores
        model_results[metric_name] = {
            'mean': scores.mean(),
            'std': scores.std()
        }
    results[name] = model_results

# æ˜¾ç¤ºç»“æœ
for model_name, model_results in results.items():
    print(f"\n{model_name}:")
    for metric_name, values in model_results.items():
        print(f"  {metric_name}: {values['mean']:.4f} Â± {values['std']:.4f}")

# å¯è§†åŒ–æ¯”è¾ƒ
plt.figure(figsize=(12, 8))
model_names = list(results.keys())
x = np.arange(len(model_names))
width = 0.25

fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))

# MSE
mse_means = [results[name]['MSE']['mean'] for name in model_names]
mse_stds = [results[name]['MSE']['std'] for name in model_names]
ax1.bar(x, mse_means, width, yerr=mse_stds, label='MSE', color='red', alpha=0.7)
ax1.set_ylabel('MSE')
ax1.set_title('å‡æ–¹è¯¯å·®(MSE)æ¯”è¾ƒ')
ax1.set_xticks(x)
ax1.set_xticklabels(model_names, rotation=45, ha='right')

# MAE
mae_means = [results[name]['MAE']['mean'] for name in model_names]
mae_stds = [results[name]['MAE']['std'] for name in model_names]
ax2.bar(x, mae_means, width, yerr=mae_stds, label='MAE', color='blue', alpha=0.7)
ax2.set_ylabel('MAE')
ax2.set_title('å¹³å‡ç»å¯¹è¯¯å·®(MAE)æ¯”è¾ƒ')
ax2.set_xticks(x)
ax2.set_xticklabels(model_names, rotation=45, ha='right')

# RÂ²
r2_means = [results[name]['RÂ²']['mean'] for name in model_names]
r2_stds = [results[name]['RÂ²']['std'] for name in model_names]
ax3.bar(x, r2_means, width, yerr=r2_stds, label='RÂ²', color='green', alpha=0.7)
ax3.set_ylabel('RÂ²')
ax3.set_title('å†³å®šç³»æ•°(RÂ²)æ¯”è¾ƒ')
ax3.set_xticks(x)
ax3.set_xticklabels(model_names, rotation=45, ha='right')

fig.tight_layout()
plt.show()
```

  </div>
</div>

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">âš ï¸</span>å¸¸è§è¯¯åŒº
  </div>
  <div class="knowledge-card__content">
    <ul>
      <li><strong>ä»…ä¾èµ–å•ä¸€æŒ‡æ ‡</strong>ï¼šä¸åŒæŒ‡æ ‡åæ˜ æ¨¡å‹æ€§èƒ½çš„ä¸åŒæ–¹é¢</li>
      <li><strong>å¿½ç•¥äº¤å‰éªŒè¯</strong>ï¼šå•æ¬¡è®­ç»ƒ/æµ‹è¯•åˆ†å‰²å¯èƒ½å¯¼è‡´é«˜æ–¹å·®ä¼°è®¡</li>
      <li><strong>è¿‡åº¦è§£è¯»RÂ²</strong>ï¼šé«˜RÂ²ä¸ä¸€å®šæ„å‘³ç€æ¨¡å‹é¢„æµ‹èƒ½åŠ›å¼º</li>
      <li><strong>å¿½ç•¥é¢†åŸŸçŸ¥è¯†</strong>ï¼šé€‰æ‹©æŒ‡æ ‡æ—¶åº”è€ƒè™‘ä¸šåŠ¡éœ€æ±‚</li>
    </ul>
  </div>
</div>

## å°ç»“ä¸æ€è€ƒ

å›å½’è¯„ä¼°æŒ‡æ ‡æ˜¯é€‰æ‹©å’Œä¼˜åŒ–å›å½’æ¨¡å‹çš„é‡è¦å·¥å…·ï¼Œä¸åŒæŒ‡æ ‡é€‚ç”¨äºä¸åŒåœºæ™¯ã€‚

### å…³é”®è¦ç‚¹å›é¡¾

- MSEå’ŒRMSEå¯¹è¾ƒå¤§è¯¯å·®ç»™äºˆæ›´é«˜æƒ©ç½šï¼Œé€‚ç”¨äºå¼‚å¸¸å€¼å½±å“è¾ƒå¤§çš„åœºæ™¯
- MAEå’ŒMedAEå¯¹å¼‚å¸¸å€¼è¾ƒä¸æ•æ„Ÿï¼Œæä¾›æ›´ç¨³å¥çš„è¯„ä¼°
- RÂ²å’Œè°ƒæ•´RÂ²è¡¡é‡æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ï¼Œä¾¿äºæ¯”è¾ƒä¸åŒå°ºåº¦çš„é—®é¢˜
- äº¤å‰éªŒè¯æä¾›æ›´å¯é çš„æ€§èƒ½ä¼°è®¡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
- å­¦ä¹ æ›²çº¿å¸®åŠ©è¯Šæ–­æ¨¡å‹æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ

### æ€è€ƒé—®é¢˜

1. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©MSEè€ŒéMAEä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Ÿ
2. ä¸ºä»€ä¹ˆRÂ²å¯èƒ½ä¸ºè´Ÿå€¼ï¼Œè¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿ
3. å¦‚ä½•æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©æœ€åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡ï¼Ÿ

<div class="practice-link">
  <a href="/projects/regression.html" class="button">å‰å¾€å®è·µé¡¹ç›®</a>
</div> 