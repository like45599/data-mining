# å†³ç­–æ ‘ç®—æ³•

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">ğŸ“š</span>æœ¬èŠ‚è¦ç‚¹
  </div>
  <div class="knowledge-card__content">
    <ul>
      <li>ç†è§£å†³ç­–æ ‘çš„åŸºæœ¬åŸç†å’Œæ„å»ºè¿‡ç¨‹</li>
      <li>æŒæ¡å†³ç­–æ ‘çš„åˆ†è£‚æ ‡å‡†å’Œå‰ªææŠ€æœ¯</li>
      <li>å­¦ä¹ å†³ç­–æ ‘åœ¨åˆ†ç±»å’Œå›å½’é—®é¢˜ä¸­çš„åº”ç”¨</li>
      <li>äº†è§£å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹åŠå…¶æ”¹è¿›æ–¹æ³•</li>
    </ul>
  </div>
</div>

## å†³ç­–æ ‘åŸºæœ¬åŸç†

å†³ç­–æ ‘æ˜¯ä¸€ç§æ ‘å½¢ç»“æ„çš„åˆ†ç±»å’Œå›å½’æ¨¡å‹ï¼Œé€šè¿‡ä¸€ç³»åˆ—é—®é¢˜å°†æ•°æ®åˆ’åˆ†ä¸ºä¸åŒçš„å­é›†ï¼Œæœ€ç»ˆå¾—åˆ°é¢„æµ‹ç»“æœã€‚

### å†³ç­–æ ‘ç»“æ„

å†³ç­–æ ‘ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆï¼š

- **æ ¹èŠ‚ç‚¹**ï¼šåŒ…å«æ‰€æœ‰æ ·æœ¬çš„èµ·å§‹èŠ‚ç‚¹
- **å†…éƒ¨èŠ‚ç‚¹**ï¼šè¡¨ç¤ºç‰¹å¾çš„æµ‹è¯•æ¡ä»¶
- **åˆ†æ”¯**ï¼šè¡¨ç¤ºæµ‹è¯•æ¡ä»¶çš„ç»“æœ
- **å¶èŠ‚ç‚¹**ï¼šè¡¨ç¤ºæœ€ç»ˆçš„åˆ†ç±»æˆ–å›å½’ç»“æœ

<div class="visualization-container">
  <div class="visualization-title">å†³ç­–æ ‘ç»“æ„ç¤ºä¾‹</div>
  <div class="visualization-content">
    <img src="/images/decision_tree_structure.svg" alt="å†³ç­–æ ‘ç»“æ„ç¤ºä¾‹">
  </div>
  <div class="visualization-caption">
    å›¾: ä¸€ä¸ªç®€å•çš„å†³ç­–æ ‘ç¤ºä¾‹ã€‚æ ¹èŠ‚ç‚¹å’Œå†…éƒ¨èŠ‚ç‚¹è¡¨ç¤ºç‰¹å¾æµ‹è¯•ï¼Œåˆ†æ”¯è¡¨ç¤ºæµ‹è¯•ç»“æœï¼Œå¶èŠ‚ç‚¹è¡¨ç¤ºæœ€ç»ˆåˆ†ç±»ã€‚
  </div>
</div>

### å†³ç­–æ ‘æ„å»ºè¿‡ç¨‹

å†³ç­–æ ‘çš„æ„å»ºé€šå¸¸é‡‡ç”¨è‡ªé¡¶å‘ä¸‹çš„é€’å½’æ–¹å¼ï¼Œä¸»è¦æ­¥éª¤åŒ…æ‹¬ï¼š

1. **é€‰æ‹©æœ€ä½³ç‰¹å¾**ï¼šä½¿ç”¨ä¿¡æ¯å¢ç›Šã€åŸºå°¼ä¸çº¯åº¦ç­‰æŒ‡æ ‡é€‰æ‹©æœ€ä½³åˆ†è£‚ç‰¹å¾
2. **åˆ†è£‚æ•°æ®é›†**ï¼šæ ¹æ®é€‰å®šçš„ç‰¹å¾å°†æ•°æ®é›†åˆ†ä¸ºå­é›†
3. **é€’å½’æ„å»ºå­æ ‘**ï¼šå¯¹æ¯ä¸ªå­é›†é‡å¤ä¸Šè¿°è¿‡ç¨‹
4. **ç¡®å®šåœæ­¢æ¡ä»¶**ï¼šå½“æ»¡è¶³ç‰¹å®šæ¡ä»¶ï¼ˆå¦‚è¾¾åˆ°æœ€å¤§æ·±åº¦ã€èŠ‚ç‚¹æ ·æœ¬æ•°è¿‡å°‘ï¼‰æ—¶åœæ­¢åˆ†è£‚

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">ğŸ’¡</span>ä½ çŸ¥é“å—ï¼Ÿ
  </div>
  <div class="knowledge-card__content">
    <p>å†³ç­–æ ‘ç®—æ³•çš„æ—©æœŸç‰ˆæœ¬CART(Classification and Regression Trees)ç”±Leo Breimanç­‰äººäº1984å¹´æå‡ºã€‚è€Œæ›´æ—©çš„ID3ç®—æ³•åˆ™ç”±Ross Quinlanåœ¨1979å¹´å¼€å‘ã€‚å†³ç­–æ ‘ä¸ä»…æ˜¯ä¸€ç§å¼ºå¤§çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä¹Ÿæ˜¯éšæœºæ£®æ—ã€æ¢¯åº¦æå‡æ ‘ç­‰é›†æˆæ–¹æ³•çš„åŸºç¡€ã€‚</p>
  </div>
</div>

## åˆ†è£‚æ ‡å‡†

å†³ç­–æ ‘ç®—æ³•çš„æ ¸å¿ƒæ˜¯å¦‚ä½•é€‰æ‹©æœ€ä½³åˆ†è£‚ç‰¹å¾å’Œåˆ†è£‚ç‚¹ã€‚å¸¸ç”¨çš„åˆ†è£‚æ ‡å‡†åŒ…æ‹¬ï¼š

### 1. ä¿¡æ¯å¢ç›Šï¼ˆID3ç®—æ³•ï¼‰

åŸºäºä¿¡æ¯ç†µçš„å‡å°‘é‡ï¼Œç†µè¡¨ç¤ºç³»ç»Ÿçš„ä¸ç¡®å®šæ€§ï¼š

$$\text{Entropy}(S) = -\sum_{i=1}^{c} p_i \log_2 p_i$$

å…¶ä¸­$p_i$æ˜¯ç±»åˆ«$i$çš„æ ·æœ¬æ¯”ä¾‹ï¼Œ$c$æ˜¯ç±»åˆ«æ•°é‡ã€‚

ä¿¡æ¯å¢ç›Šè®¡ç®—ä¸ºï¼š

$$\text{Gain}(S, A) = \text{Entropy}(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Entropy}(S_v)$$

å…¶ä¸­$A$æ˜¯ç‰¹å¾ï¼Œ$S_v$æ˜¯ç‰¹å¾$A$å–å€¼ä¸º$v$çš„æ ·æœ¬å­é›†ã€‚

### 2. å¢ç›Šç‡ï¼ˆC4.5ç®—æ³•ï¼‰

ä¸ºå…‹æœä¿¡æ¯å¢ç›Šåå‘å¤šå€¼ç‰¹å¾çš„ç¼ºç‚¹ï¼ŒC4.5å¼•å…¥äº†å¢ç›Šç‡ï¼š

$$\text{GainRatio}(S, A) = \frac{\text{Gain}(S, A)}{\text{SplitInfo}(S, A)}$$

å…¶ä¸­ï¼š

$$\text{SplitInfo}(S, A) = -\sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}$$

### 3. åŸºå°¼ä¸çº¯åº¦ï¼ˆCARTç®—æ³•ï¼‰

è¡¡é‡é›†åˆçš„ä¸çº¯åº¦ï¼Œå€¼è¶Šå°è¡¨ç¤ºé›†åˆä¸­çš„æ ·æœ¬è¶Šçº¯å‡€ï¼š

$$\text{Gini}(S) = 1 - \sum_{i=1}^{c} p_i^2$$

åŸºå°¼æŒ‡æ•°è®¡ç®—ä¸ºï¼š

$$\text{Gini\_Index}(S, A) = \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \text{Gini}(S_v)$$

é€‰æ‹©ä½¿åŸºå°¼æŒ‡æ•°æœ€å°çš„ç‰¹å¾ä½œä¸ºæœ€ä½³åˆ†è£‚ç‰¹å¾ã€‚

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import tree
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åˆ›å»ºå¹¶è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹ï¼ˆä½¿ç”¨åŸºå°¼ä¸çº¯åº¦ï¼‰
dt_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
dt_gini.fit(X_train, y_train)

# åˆ›å»ºå¹¶è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹ï¼ˆä½¿ç”¨ä¿¡æ¯å¢ç›Šï¼‰
dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
dt_entropy.fit(X_train, y_train)

# é¢„æµ‹
y_pred_gini = dt_gini.predict(X_test)
y_pred_entropy = dt_entropy.predict(X_test)

# è¯„ä¼°
gini_accuracy = dt_gini.score(X_test, y_test)
entropy_accuracy = dt_entropy.score(X_test, y_test)

print(f"åŸºå°¼ä¸çº¯åº¦å†³ç­–æ ‘å‡†ç¡®ç‡: {gini_accuracy:.4f}")
print(f"ä¿¡æ¯å¢ç›Šå†³ç­–æ ‘å‡†ç¡®ç‡: {entropy_accuracy:.4f}")

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(15, 10))
tree.plot_tree(dt_gini, feature_names=iris.feature_names, 
               class_names=iris.target_names, filled=True)
plt.title("åŸºå°¼ä¸çº¯åº¦å†³ç­–æ ‘")
plt.show()
```

  </div>
</div>

## å†³ç­–æ ‘å‰ªæ

å†³ç­–æ ‘å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå‰ªææ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆçš„é‡è¦æŠ€æœ¯ã€‚

### é¢„å‰ªæ

åœ¨æ„å»ºè¿‡ç¨‹ä¸­é¢„å…ˆåœæ­¢æ ‘çš„ç”Ÿé•¿ï¼š

- é™åˆ¶æ ‘çš„æœ€å¤§æ·±åº¦
- é™åˆ¶èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°
- é™åˆ¶åˆ†è£‚çš„æœ€å°ä¿¡æ¯å¢ç›Š

### åå‰ªæ

å…ˆæ„å»ºå®Œæ•´æ ‘ï¼Œç„¶åè‡ªåº•å‘ä¸Šå‰ªå»ä¸é‡è¦çš„å­æ ‘ï¼š

- é”™è¯¯ç‡é™ä½å‰ªæ
- ä»£ä»·å¤æ‚åº¦å‰ªæï¼ˆCARTç®—æ³•ä½¿ç”¨ï¼‰

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# åŠ è½½æ•°æ®
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'max_depth': [None, 3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [None, 'sqrt', 'log2'],
    'ccp_alpha': [0.0, 0.01, 0.02, 0.03, 0.04]  # ä»£ä»·å¤æ‚åº¦å‚æ•°ï¼Œç”¨äºåå‰ªæ
}

# åˆ›å»ºç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# æ‰§è¡Œç½‘æ ¼æœç´¢
grid_search.fit(X_train, y_train)

# è¾“å‡ºæœ€ä½³å‚æ•°
print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹
best_model = grid_search.best_estimator_
test_score = best_model.score(X_test, y_test)
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")

# å¯è§†åŒ–æœ€ä½³å†³ç­–æ ‘
plt.figure(figsize=(15, 10))
tree.plot_tree(best_model, feature_names=cancer.feature_names, 
               class_names=['æ¶æ€§', 'è‰¯æ€§'], filled=True, max_depth=3)
plt.title("æœ€ä½³å†³ç­–æ ‘ï¼ˆé™åˆ¶æ˜¾ç¤ºæ·±åº¦ä¸º3ï¼‰")
plt.show()
```

  </div>
</div>

## ç‰¹å¾é‡è¦æ€§

å†³ç­–æ ‘å¯ä»¥è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼Œå¸®åŠ©ç†è§£å“ªäº›ç‰¹å¾å¯¹é¢„æµ‹æœ€æœ‰å½±å“ï¼š

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
import pandas as pd
import matplotlib.pyplot as plt

# ä½¿ç”¨ä¹‹å‰è®­ç»ƒçš„æœ€ä½³æ¨¡å‹
feature_importance = best_model.feature_importances_

# åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
importance_df = pd.DataFrame({
    'feature': cancer.feature_names,
    'importance': feature_importance
})

# æŒ‰é‡è¦æ€§æ’åº
importance_df = importance_df.sort_values('importance', ascending=False)

# å¯è§†åŒ–å‰15ä¸ªé‡è¦ç‰¹å¾
plt.figure(figsize=(12, 8))
plt.barh(importance_df['feature'][:15], importance_df['importance'][:15])
plt.xlabel('é‡è¦æ€§')
plt.ylabel('ç‰¹å¾')
plt.title('å†³ç­–æ ‘ç‰¹å¾é‡è¦æ€§')
plt.gca().invert_yaxis()  # ä»ä¸Šåˆ°ä¸‹æ˜¾ç¤º
plt.show()
```

  </div>
</div>

## å†³ç­–æ ‘å›å½’

å†³ç­–æ ‘ä¸ä»…å¯ç”¨äºåˆ†ç±»ï¼Œä¹Ÿå¯ç”¨äºå›å½’é—®é¢˜ï¼š

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# åŠ è½½æ•°æ®
boston = load_boston()
X, y = boston.data, boston.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åˆ›å»ºå¹¶è®­ç»ƒå†³ç­–æ ‘å›å½’æ¨¡å‹
regressor = DecisionTreeRegressor(max_depth=5, random_state=42)
regressor.fit(X_train, y_train)

# é¢„æµ‹
y_pred = regressor.predict(X_test)

# è¯„ä¼°
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"å‡æ–¹è¯¯å·®(MSE): {mse:.4f}")
print(f"å‡æ–¹æ ¹è¯¯å·®(RMSE): {rmse:.4f}")
print(f"å†³å®šç³»æ•°(RÂ²): {r2:.4f}")

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('å®é™…å€¼')
plt.ylabel('é¢„æµ‹å€¼')
plt.title('å†³ç­–æ ‘å›å½’ï¼šå®é™…å€¼ vs é¢„æµ‹å€¼')
plt.show()
```

  </div>
</div>

## å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹

### ä¼˜ç‚¹

1. **æ˜“äºç†è§£å’Œè§£é‡Š**ï¼šå†³ç­–æ ‘çš„å†³ç­–è¿‡ç¨‹ç›´è§‚å¯è§†åŒ–
2. **æ— éœ€ç‰¹å¾ç¼©æ”¾**ï¼šå¯¹ç‰¹å¾å°ºåº¦ä¸æ•æ„Ÿ
3. **èƒ½å¤„ç†æ•°å€¼å’Œç±»åˆ«ç‰¹å¾**ï¼šæ— éœ€ç‰¹æ®Šç¼–ç 
4. **èƒ½å¤„ç†å¤šåˆ†ç±»é—®é¢˜**ï¼šè‡ªç„¶æ”¯æŒå¤šç±»åˆ«åˆ†ç±»
5. **å¯ä»¥å¤„ç†ç¼ºå¤±å€¼**ï¼šèƒ½å¤Ÿå­¦ä¹ å¤„ç†ç¼ºå¤±å€¼çš„æ¨¡å¼

### ç¼ºç‚¹

1. **å®¹æ˜“è¿‡æ‹Ÿåˆ**ï¼šç‰¹åˆ«æ˜¯æ ‘æ·±åº¦è¾ƒå¤§æ—¶
2. **ä¸ç¨³å®š**ï¼šæ•°æ®å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´æ ‘ç»“æ„æ˜¾è‘—å˜åŒ–
3. **åå‘äºé«˜åŸºæ•°ç‰¹å¾**ï¼šå€¾å‘äºé€‰æ‹©å–å€¼è¾ƒå¤šçš„ç‰¹å¾
4. **éš¾ä»¥å­¦ä¹ æŸäº›å…³ç³»**ï¼šå¦‚XORå…³ç³»
5. **é¢„æµ‹æ€§èƒ½æœ‰é™**ï¼šå•æ£µæ ‘çš„é¢„æµ‹èƒ½åŠ›é€šå¸¸ä¸å¦‚é›†æˆæ–¹æ³•

<div class="knowledge-card">
  <div class="knowledge-card__title">
    <span class="icon">âš ï¸</span>å¸¸è§è¯¯åŒº
  </div>
  <div class="knowledge-card__content">
    <ul>
      <li><strong>å¿½ç•¥å‰ªæ</strong>ï¼šæœªè¿›è¡Œé€‚å½“å‰ªæå¯¼è‡´è¿‡æ‹Ÿåˆ</li>
      <li><strong>è¿‡åº¦ä¾èµ–å•æ£µæ ‘</strong>ï¼šåœ¨å¤æ‚é—®é¢˜ä¸Šæœªè€ƒè™‘ä½¿ç”¨é›†æˆæ–¹æ³•</li>
      <li><strong>å¿½ç•¥ç±»åˆ«ä¸å¹³è¡¡</strong>ï¼šæœªè®¾ç½®class_weightå‚æ•°å¤„ç†ä¸å¹³è¡¡æ•°æ®</li>
      <li><strong>é”™è¯¯è§£è¯»ç‰¹å¾é‡è¦æ€§</strong>ï¼šç‰¹å¾é‡è¦æ€§ä¸ç­‰åŒäºå› æœå…³ç³»</li>
    </ul>
  </div>
</div>

## å†³ç­–æ ‘çš„æ”¹è¿›ä¸æ‰©å±•

ä¸ºå…‹æœå†³ç­–æ ‘çš„å±€é™æ€§ï¼Œå‡ºç°äº†å¤šç§æ”¹è¿›å’Œæ‰©å±•æ–¹æ³•ï¼š

1. **éšæœºæ£®æ—**ï¼šæ„å»ºå¤šæ£µæ ‘å¹¶å–å¹³å‡/å¤šæ•°æŠ•ç¥¨
2. **æ¢¯åº¦æå‡æ ‘**ï¼šé€šè¿‡æ¢¯åº¦æå‡æ–¹æ³•ä¸²è¡Œæ„å»ºæ ‘
3. **XGBoost/LightGBM**ï¼šé«˜æ•ˆå®ç°çš„æ¢¯åº¦æå‡æ ‘åº“

<div class="code-example">
  <div class="code-example__title">ä»£ç ç¤ºä¾‹</div>
  <div class="code-example__content">

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# åˆ›å»ºå¹¶è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# é¢„æµ‹
y_pred = rf.predict(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"éšæœºæ£®æ—å‡†ç¡®ç‡: {accuracy:.4f}")

# ä¸å•æ£µå†³ç­–æ ‘æ¯”è¾ƒ
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print(f"å•æ£µå†³ç­–æ ‘å‡†ç¡®ç‡: {dt_accuracy:.4f}")
```

  </div>
</div>

## å°ç»“ä¸æ€è€ƒ

å†³ç­–æ ‘æ˜¯ä¸€ç§ç›´è§‚ä¸”å¼ºå¤§çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¢å¯ç”¨äºåˆ†ç±»ä¹Ÿå¯ç”¨äºå›å½’ã€‚å°½ç®¡å•æ£µå†³ç­–æ ‘æœ‰ä¸€å®šå±€é™æ€§ï¼Œä½†å®ƒæ˜¯è®¸å¤šé«˜çº§é›†æˆæ–¹æ³•çš„åŸºç¡€ã€‚

### å…³é”®è¦ç‚¹å›é¡¾

- å†³ç­–æ ‘é€šè¿‡é€’å½’åˆ†è£‚æ•°æ®æ„å»ºæ ‘å½¢ç»“æ„
- å¸¸ç”¨åˆ†è£‚æ ‡å‡†åŒ…æ‹¬ä¿¡æ¯å¢ç›Šã€å¢ç›Šç‡å’ŒåŸºå°¼ä¸çº¯åº¦
- å‰ªææŠ€æœ¯å¯¹é˜²æ­¢è¿‡æ‹Ÿåˆè‡³å…³é‡è¦
- å†³ç­–æ ‘å¯ä»¥è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼Œæä¾›æ¨¡å‹è§£é‡Šæ€§
- éšæœºæ£®æ—ç­‰é›†æˆæ–¹æ³•å¯ä»¥å…‹æœå•æ£µæ ‘çš„å±€é™æ€§

### æ€è€ƒé—®é¢˜

1. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©å†³ç­–æ ‘è€Œéå…¶ä»–åˆ†ç±»/å›å½’ç®—æ³•ï¼Ÿ
2. å¦‚ä½•å¹³è¡¡å†³ç­–æ ‘çš„å¤æ‚åº¦å’Œé¢„æµ‹æ€§èƒ½ï¼Ÿ
3. ä¸ºä»€ä¹ˆéšæœºæ£®æ—é€šå¸¸æ¯”å•æ£µå†³ç­–æ ‘è¡¨ç°æ›´å¥½ï¼Ÿ

<BackToPath />

<div class="practice-link">
  <a href="/projects/classification.html" class="button">å‰å¾€å®è·µé¡¹ç›®</a>
</div> 